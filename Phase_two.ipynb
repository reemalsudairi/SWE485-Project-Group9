{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba7a1f8-36f5-468d-a7e1-75bb8ac49835",
   "metadata": {},
   "source": [
    "## Performance Evaluation Metrics  \n",
    "\n",
    "To assess the effectiveness of our models, we need to use appropriate performance measures. In this case, we have chosen accuracy score and classification report as our primary evaluation metrics.  \n",
    "\n",
    "## Why These Metrics?\n",
    "\n",
    "### Accuracy Score\n",
    "- Accuracy provides a straightforward measure of overall model performance by calculating the proportion of correct predictions out of all predictions.  \n",
    "- Since we are dealing with a binary classification problem, accuracy serves as a useful baseline metric to compare the models.  \n",
    "- This metric is particularly relevant when class distribution is not highly imbalanced, allowing us to assess how well the models differentiate between similar and non-similar properties.\n",
    " \n",
    "It is defined as:  \n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "\n",
    "### Classification Report\n",
    "While accuracy gives a general performance overview, it does not reveal how well the model handles each class. The classification report provides a more detailed evaluation through:  \n",
    "\n",
    "- Precision â€“ Measures how many of the predicted similar properties were actually similar, helping assess false positives. It is defined as:  \n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "A high precision score indicates that the model makes few false positive predictions.  \n",
    "\n",
    "- Recall â€“ Measures how well the model identifies all similar properties, ensuring we minimize false negatives. It is defined as:  \n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "A high recall score means the model is good at identifying positive instances.  \n",
    "\n",
    "- F1-Score â€“ Balances precision and recall, making it useful if there are slight class imbalances. It is defined as:  \n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "A high F1-score indicates a good balance between precision and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffe2a6-ca1c-4d8b-af39-7ae4fb7bbce5",
   "metadata": {},
   "source": [
    "\n",
    "### K-Nearest Neighbors and Random Forest in Property Recommendation Systems\n",
    "\n",
    "Since our goal of the recommendation system is to suggest properties based on two numerical \n",
    "user inputs Price and Area and the output is a list of similar properties matching the criteria, \n",
    "including details such as: Property Type (Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±) Location (Ø§Ù„Ù…ÙˆÙ‚Ø¹) District (Ø§Ù„Ø­ÙŠ) Bedrooms (Ø§Ù„ØºØ±Ù) \n",
    "Bathrooms (Ø¯ÙˆØ±Ø§Øª Ø§Ù„Ù…ÙŠØ§Ù‡) Price (Ø§Ù„Ø³Ø¹Ø±) Agency Name , we want the output to be a search for the \n",
    "most similar properties not a prediction task.\n",
    "\n",
    "#### K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN is a simple, non-parametric machine learning algorithm used for classification and regression. It operates by identifying the k closest data points to a given input and then assigning the most frequent class for classification or calculating the average for regression[1].\n",
    "\n",
    "##### Why K-Nearest Neighbors (KNN) is Best for Property Recommendation Systems?\n",
    "\n",
    "KNN is a great choice for a property recommendation system for several reasons. Unlike other \n",
    "models that try to predict a specific price or property, like Linear Regression, KNN focuses on \n",
    "finding the most similar properties according to user inputs such as price and area.\n",
    "When a user specifies their desired price and area, KNN locates the most similar properties in the dataset through a series of steps: first, it chooses a value for k, which determines how many similar properties to find; next, it measures the similarity of each property to the user's input using a distance metric; then, it identifies the k nearest neighbors that best match the specified price and area; finally, it returns a list of the k most relevant properties, providing multiple options rather than just one. This makes KNN an effective method for recommendation systems, as it offers users a range of property choices that best match their criteria [1].\n",
    "\n",
    "##### Why K-Nearest Neighbors (KNN) is Best for This Dataset?\n",
    "\n",
    "By observing the dataset, we can see that certain features are more closely related, which affects how KNN determines similarity among properties. For example, there's a strong correlation between the number of bedrooms and bathrooms, showing that properties with more bedrooms usually have more bathrooms. Additionally, the price tends to be more closely related to the number of bedrooms and bathrooms, indicating that properties with more rooms typically have higher prices [2].\n",
    "\n",
    "#### Random Forest (RF)\n",
    "\n",
    "Random Forest is a robust machine learning model that is particularly effective at analyzing structured real estate data and providing accurate predictions. It functions as an ensemble of multiple decision trees, with each tree learning from different subsets of data to create a more balanced and generalized recommendation system. This approach enables the model to effectively capture the intricate relationships between \"Ø§Ù„Ø³Ø¹Ø±\" (price) and \"Ø§Ù„Ù…Ø³Ø§Ø­Ø©\" (area), ensuring that users receive property suggestions that align with their financial and spatial preferences. Unlike traditional models that assume a linear relationship between price and area, Random Forest can identify patterns and non-linear trends, making it highly suitable for the complexities of real-world real estate markets [3]. \n",
    "\n",
    "##### Why Random Forest is Suitable for Recommendations?\n",
    "\n",
    "When it comes to property recommendations, \"Ø§Ù„Ø³Ø¹Ø±\" and \"Ø§Ù„Ù…Ø³Ø§Ø­Ø©\" are among the most important factors that affect user choices. Random Forest emphasizes these attributes through its feature selection process, allowing it to effectively filter and rank properties according to user needs. Given that real estate pricing is affected by various factors, the model's capability to combine insights from multiple decision trees ensures it delivers reliable and balanced recommendations. Additionally, its strength in handling outliers enables it to produce valuable suggestions, making it a practical and robust option for recommendation systems. [4].\n",
    "\n",
    "##### Why Random Forest is Suitable for This Dataset?\n",
    "\n",
    "In real estate, the connection between \"Ø§Ù„Ø³Ø¹Ø±\" (price) and \"Ø§Ù„Ù…Ø³Ø§Ø­Ø©\" (area) can differ greatly depending on \"Ø§Ù„Ù…ÙˆÙ‚Ø¹\" (location) and \"Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±\" (property type). Random Forest effectively captures these variations by using these attributes as features, enabling the model to identify unique pricing trends for various areas and property types. By adjusting its decision-making based on these differences, Random Forest ensures that property recommendations are both accurate and relevant, whether someone is looking for an apartment, villa, or commercial space in different locations. This flexibility makes it especially suitable for real estate datasets that involve numerous influencing factors. [4].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f121bd-17a6-4686-981b-07d1a91cc26d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset/cleaned_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m     11\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset/cleaned_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Drop the \"Property_ID\" column as it's an identifier not useful for modeling.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProperty_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset/cleaned_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Dataset/cleaned_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the \"Property_ID\" column as it's an identifier not useful for modeling.\n",
    "df = df.drop(columns=[\"Property_ID\"], errors='ignore')\n",
    "\n",
    "# Define the numerical features which we will use for analysis and predictions.\n",
    "num_features = [\"Area\", \"Price\"]\n",
    "\n",
    "# These columns will keep the original, unscaled values to display later in the recommendations.\n",
    "df[\"Original_Area\"] = df[\"Area\"]\n",
    "df[\"Original_Price\"] = df[\"Price\"]\n",
    "\n",
    "# A preprocessing pipeline is created using a StandardScaler to normalize the 'Area' and 'Price' columns for consistent scaling of the data.\n",
    "preprocessor = Pipeline([\n",
    "    ('scaler', StandardScaler())  # This scales the features to have zero mean and unit variance.\n",
    "])\n",
    "\n",
    "# Normalize the numerical features (Area and Price).\n",
    "df[num_features] = preprocessor.fit_transform(df[num_features])\n",
    "\n",
    "# Generate similarity labels for training. This function assigns a 'Similarity_Label' to properties based on their proximity to each other in price and area.\n",
    "df[\"Similarity_Label\"] = 0  # Default label is \"Not similar\".\n",
    "\n",
    "def assign_similarity_labels(df, threshold=0.1):\n",
    "    for i, row in df.iterrows():\n",
    "        area, price = row[\"Area\"], row[\"Price\"]\n",
    "        # Calculate the Euclidean distance between this property and all others.\n",
    "        distances = np.sqrt((df[\"Area\"] - area) ** 2 + (df[\"Price\"] - price) ** 2)\n",
    "        closest_indices = distances.nsmallest(3).index  # Adjusted to 2 closest + itself.\n",
    "        df.loc[closest_indices, \"Similarity_Label\"] = 1  # Mark these properties as similar.\n",
    "\n",
    "assign_similarity_labels(df)\n",
    "\n",
    "# We will split the data into a training set (80%) and a testing set (20%) for model evaluation.\n",
    "X = df[num_features]\n",
    "y = df[\"Similarity_Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# We will train three models: KNN, Random Forest, and Gradient Boosting.\n",
    "models = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights=\"distance\"),  # Enable probability-based similarity\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "\n",
    "# Train each model and evaluate it using accuracy and classification report.\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Train the model on the training set.\n",
    "    y_pred = model.predict(X_test)  # Make predictions on the test set.\n",
    "    \n",
    "    # Output the accuracy score and classification report for each model with clear separation\n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(f\" {name} Performance Metrics:\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"{'-'*100}\")\n",
    "\n",
    "# This function will return a list of recommended properties based on the price and area specified by the user.\n",
    "def recommend_properties(price, area, model_name, top_n=5):\n",
    "    input_data = pd.DataFrame([[area, price]], columns=num_features)\n",
    "    input_data_scaled = pd.DataFrame(preprocessor.transform(input_data), columns=num_features)\n",
    "\n",
    "    model = models.get(model_name)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"ğŸš¨ Model '{model_name}' not found!\")\n",
    "        return None\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):  \n",
    "        # **Use only the input property for prediction**\n",
    "        input_prob = model.predict_proba(input_data_scaled)[:, 1]\n",
    "\n",
    "        # **Predict probabilities for all properties compared to input**\n",
    "        probabilities = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    elif isinstance(model, KNeighborsClassifier):  \n",
    "        # **For KNN: Find closest properties to the input**\n",
    "        distances, indices = model.kneighbors(input_data_scaled, n_neighbors=min(top_n + 5, len(X)))  \n",
    "        probabilities = np.zeros(len(X))  \n",
    "        probabilities[indices.flatten()] = 1 / (distances.flatten() + 1e-5)  \n",
    "\n",
    "    else:\n",
    "        # **Fallback: Use Euclidean distance manually**\n",
    "        probabilities = -np.sqrt((df[\"Area\"] - input_data_scaled.iloc[0, 0]) ** 2 + (df[\"Price\"] - input_data_scaled.iloc[0, 1]) ** 2)\n",
    "\n",
    "    # **Sort based on highest similarity to input**\n",
    "    recommended_indices = np.argsort(-probabilities)[:top_n]\n",
    "    recommended = df.iloc[recommended_indices].copy()\n",
    "\n",
    "    # **Rename columns to Arabic**\n",
    "    recommended = recommended.rename(columns={\n",
    "        \"Property Type\": \"Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±\",\n",
    "        \"Location\": \"Ø§Ù„Ù…ÙˆÙ‚Ø¹\",\n",
    "        \"District\": \"Ø§Ù„Ø­ÙŠ\",\n",
    "        \"Bedrooms\": \"Ø§Ù„ØºØ±Ù\",\n",
    "        \"Bathrooms\": \"Ø¯ÙˆØ±Ø§Øª Ø§Ù„Ù…ÙŠØ§Ø©\",\n",
    "        \"Original_Area\": \"Ø§Ù„Ù…Ø³Ø§Ø­Ø©\",\n",
    "        \"Original_Price\": \"Ø§Ù„Ø³Ø¹Ø±\",\n",
    "        \"Agency_Name\": \"Ø§Ù„ÙˆÙƒØ§Ù„Ø©\"\n",
    "    })\n",
    "\n",
    "    headers = [\"Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±\", \"Ø§Ù„Ù…ÙˆÙ‚Ø¹\", \"Ø§Ù„Ø­ÙŠ\", \"Ø§Ù„ØºØ±Ù\", \"Ø¯ÙˆØ±Ø§Øª Ø§Ù„Ù…ÙŠØ§Ø©\", \"Ø§Ù„Ù…Ø³Ø§Ø­Ø©\", \"Ø§Ù„Ø³Ø¹Ø±\", \"Ø§Ù„ÙˆÙƒØ§Ù„Ø©\"]\n",
    "    results = recommended[headers]\n",
    "\n",
    "    print(f\"\\n ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ¯Ù„ {model_name}:\\n\")\n",
    "    print(\" | \".join(headers))\n",
    "    print(\"-\" * 100)\n",
    "    for _, row in results.iterrows():\n",
    "        print(\" | \".join(str(x) for x in row.values))\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage of the agar recommendation system:\n",
    "price_input = 1000\n",
    "area_input = 300\n",
    "\n",
    "# Loop through all models and print recommendations for each one.\n",
    "for model_name in models.keys():\n",
    "    recommend_properties(price_input, area_input, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84c0e0-f676-4333-a2e8-3c39b398d1ba",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] GeeksforGeeks, \"Recommender Systems using KNN,\" GeeksforGeeks, Feb. 27, 2025. [Online]. Available: https://www.geeksforgeeks.org/recommender-systems-using-knn/?ref=asr9. [Accessed: Feb. 27, 2025].\n",
    "\n",
    "[2] R. K. Halder, M. N. Uddin, M. A. Uddin, S. Aryal, and A. Khraisat, \"Enhancing K-nearest neighbor algorithm: a comprehensive review and performance analysis of modifications,\" Journal of Big Data, vol. 11, no. 1, pp. 1â€“55, Aug. 2024. [Online]. Available: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00973-y. [Accessed: Feb. 27, 2025].\n",
    "\n",
    "[3] E2E Networks, \"Random Forest Algorithm in Machine Learning: A Guide,\" E2E Networks Blog, [Online]. Available: https://www.e2enetworks.com/blog/random-forest-algorithm-in-machine-learning-a-guide. [Accessed: Feb. 27, 2025].\n",
    "\n",
    "[4] IBM, \"Random Forest,\" IBM Think, [Online]. Available: https://www.ibm.com/think/topics/random-forest. [Accessed: Feb. 27, 2025].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f0f5e-5abd-420b-ba09-3de842508330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
